{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA is the low ranking technique for model tuning. Try understanding what the rank means in matrices and also what is intrinsic dimensionality.\n",
    "\n",
    "In the context of linear algebra and LoRA (Low-Rank Adaptation), the concept of **rank** plays a crucial role in understanding how model parameters are optimized and adapted efficiently. Here's a breakdown:\n",
    "\n",
    "### Rank in Linear Algebra:\n",
    "- **Rank** refers to the number of linearly independent rows or columns in a matrix. In simpler terms, it indicates the dimensionality of the space spanned by the matrix's rows or columns.\n",
    "- For a matrix \\( A \\in \\mathbb{R}^{m \\times n} \\), the rank is the maximum number of linearly independent vectors you can extract from either the rows or the columns of \\( A \\). If all rows (or columns) are linearly independent, the rank is \\( \\min(m, n) \\); otherwise, it's lower.\n",
    "\n",
    "### Rank in LoRA (Low-Rank Adaptation):\n",
    "LoRA is a technique used in fine-tuning large language models (LLMs) where the idea is to adapt model weights using **low-rank matrices**. Here's how rank applies in LoRA:\n",
    "\n",
    "1. **Low-Rank Matrix Decomposition**:\n",
    "   - In large models, the full parameter matrices (like weight matrices in transformers) are very large and dense, making fine-tuning all parameters computationally expensive.\n",
    "   - LoRA uses a technique where instead of updating the entire weight matrix \\( W \\), it approximates the change in \\( W \\) using the product of two smaller matrices, \\( A \\in \\mathbb{R}^{m \\times r} \\) and \\( B \\in \\mathbb{R}^{r \\times n} \\), where \\( r \\) is much smaller than \\( m \\) and \\( n \\) (with \\( r \\) representing the rank). This is called **low-rank decomposition**.\n",
    "   \n",
    "2. **Why Low-Rank?**:\n",
    "   - In many high-dimensional datasets (like the ones LLMs are trained on), the effective rank of the weight updates is low, meaning the weight updates lie in a subspace of the full parameter space. LoRA exploits this by using low-rank matrices \\( A \\) and \\( B \\) to model these updates, significantly reducing the number of parameters that need to be fine-tuned.\n",
    "   - As a result, LoRA introduces a low-rank structure to reduce the computational burden without sacrificing much model performance.\n",
    "\n",
    "3. **Application in LLM Fine-tuning**:\n",
    "   - Instead of fine-tuning the entire weight matrix \\( W \\), LoRA updates only the smaller low-rank matrices \\( A \\) and \\( B \\). The change to \\( W \\) can be represented as \\( W + \\Delta W \\), where \\( \\Delta W = A \\times B \\).\n",
    "   - The **rank** \\( r \\) controls the complexity and expressiveness of the adaptation. A higher rank allows for more expressive updates but requires more computation and parameters, while a lower rank reduces computational complexity.\n",
    "\n",
    "In summary, **rank** in linear algebra refers to the number of linearly independent vectors in a matrix, and in LoRA, this concept is used to simplify and optimize the fine-tuning of large language models by using low-rank matrix approximations for weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/torchtune/stable/tutorials/lora_finetune.html \n",
    "# https://arxiv.org/pdf/2106.09685 (LoRA: Low-Rank Adaptation of Large Language Models)\n",
    "# https://arxiv.org/pdf/2012.13255 (INTRINSIC DIMENSIONALITY)\n",
    "\n",
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Llama2 without any LoRA layers\n",
    "base_model = llama2_7b()\n",
    "\n",
    "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
    "# We just need to define which layers we want LoRA applied to.\n",
    "# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n",
    "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
    "# layers outside of the self-attention.\n",
    "lora_model = lora_llama2_7b(lora_attn_modules=[\"q_proj\", \"v_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(base_model.layers[0].attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention(\n",
      "  (q_proj): LoRALinear(\n",
      "    (dropout): Identity()\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): LoRALinear(\n",
      "    (dropout): Identity()\n",
      "    (lora_a): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (lora_b): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model.layers[0].attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight', 'layers.6.attn.q_proj.lora_a.weight', 'layers.6.attn.q_proj.lora_b.weight', 'layers.6.attn.v_proj.lora_a.weight', 'layers.6.attn.v_proj.lora_b.weight', 'layers.7.attn.q_proj.lora_a.weight', 'layers.7.attn.q_proj.lora_b.weight', 'layers.7.attn.v_proj.lora_a.weight', 'layers.7.attn.v_proj.lora_b.weight', 'layers.8.attn.q_proj.lora_a.weight', 'layers.8.attn.q_proj.lora_b.weight', 'layers.8.attn.v_proj.lora_a.weight', 'layers.8.attn.v_proj.lora_b.weight', 'layers.9.attn.q_proj.lora_a.weight', 'layers.9.attn.q_proj.lora_b.weight', 'layers.9.attn.v_proj.lora_a.weight', 'layers.9.attn.v_proj.lora_b.weight', 'layers.10.attn.q_proj.lora_a.weight', 'layers.10.attn.q_proj.lora_b.weight', 'layers.10.attn.v_proj.lora_a.weight', 'layers.10.attn.v_proj.lora_b.weight', 'layers.11.attn.q_proj.lora_a.weight', 'layers.11.attn.q_proj.lora_b.weight', 'layers.11.attn.v_proj.lora_a.weight', 'layers.11.attn.v_proj.lora_b.weight', 'layers.12.attn.q_proj.lora_a.weight', 'layers.12.attn.q_proj.lora_b.weight', 'layers.12.attn.v_proj.lora_a.weight', 'layers.12.attn.v_proj.lora_b.weight', 'layers.13.attn.q_proj.lora_a.weight', 'layers.13.attn.q_proj.lora_b.weight', 'layers.13.attn.v_proj.lora_a.weight', 'layers.13.attn.v_proj.lora_b.weight', 'layers.14.attn.q_proj.lora_a.weight', 'layers.14.attn.q_proj.lora_b.weight', 'layers.14.attn.v_proj.lora_a.weight', 'layers.14.attn.v_proj.lora_b.weight', 'layers.15.attn.q_proj.lora_a.weight', 'layers.15.attn.q_proj.lora_b.weight', 'layers.15.attn.v_proj.lora_a.weight', 'layers.15.attn.v_proj.lora_b.weight', 'layers.16.attn.q_proj.lora_a.weight', 'layers.16.attn.q_proj.lora_b.weight', 'layers.16.attn.v_proj.lora_a.weight', 'layers.16.attn.v_proj.lora_b.weight', 'layers.17.attn.q_proj.lora_a.weight', 'layers.17.attn.q_proj.lora_b.weight', 'layers.17.attn.v_proj.lora_a.weight', 'layers.17.attn.v_proj.lora_b.weight', 'layers.18.attn.q_proj.lora_a.weight', 'layers.18.attn.q_proj.lora_b.weight', 'layers.18.attn.v_proj.lora_a.weight', 'layers.18.attn.v_proj.lora_b.weight', 'layers.19.attn.q_proj.lora_a.weight', 'layers.19.attn.q_proj.lora_b.weight', 'layers.19.attn.v_proj.lora_a.weight', 'layers.19.attn.v_proj.lora_b.weight', 'layers.20.attn.q_proj.lora_a.weight', 'layers.20.attn.q_proj.lora_b.weight', 'layers.20.attn.v_proj.lora_a.weight', 'layers.20.attn.v_proj.lora_b.weight', 'layers.21.attn.q_proj.lora_a.weight', 'layers.21.attn.q_proj.lora_b.weight', 'layers.21.attn.v_proj.lora_a.weight', 'layers.21.attn.v_proj.lora_b.weight', 'layers.22.attn.q_proj.lora_a.weight', 'layers.22.attn.q_proj.lora_b.weight', 'layers.22.attn.v_proj.lora_a.weight', 'layers.22.attn.v_proj.lora_b.weight', 'layers.23.attn.q_proj.lora_a.weight', 'layers.23.attn.q_proj.lora_b.weight', 'layers.23.attn.v_proj.lora_a.weight', 'layers.23.attn.v_proj.lora_b.weight', 'layers.24.attn.q_proj.lora_a.weight', 'layers.24.attn.q_proj.lora_b.weight', 'layers.24.attn.v_proj.lora_a.weight', 'layers.24.attn.v_proj.lora_b.weight', 'layers.25.attn.q_proj.lora_a.weight', 'layers.25.attn.q_proj.lora_b.weight', 'layers.25.attn.v_proj.lora_a.weight', 'layers.25.attn.v_proj.lora_b.weight', 'layers.26.attn.q_proj.lora_a.weight', 'layers.26.attn.q_proj.lora_b.weight', 'layers.26.attn.v_proj.lora_a.weight', 'layers.26.attn.v_proj.lora_b.weight', 'layers.27.attn.q_proj.lora_a.weight', 'layers.27.attn.q_proj.lora_b.weight', 'layers.27.attn.v_proj.lora_a.weight', 'layers.27.attn.v_proj.lora_b.weight', 'layers.28.attn.q_proj.lora_a.weight', 'layers.28.attn.q_proj.lora_b.weight', 'layers.28.attn.v_proj.lora_a.weight', 'layers.28.attn.v_proj.lora_b.weight', 'layers.29.attn.q_proj.lora_a.weight', 'layers.29.attn.q_proj.lora_b.weight', 'layers.29.attn.v_proj.lora_a.weight', 'layers.29.attn.v_proj.lora_b.weight', 'layers.30.attn.q_proj.lora_a.weight', 'layers.30.attn.q_proj.lora_b.weight', 'layers.30.attn.v_proj.lora_a.weight', 'layers.30.attn.v_proj.lora_b.weight', 'layers.31.attn.q_proj.lora_a.weight', 'layers.31.attn.q_proj.lora_b.weight', 'layers.31.attn.v_proj.lora_a.weight', 'layers.31.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming that base_model already has the pretrained Llama2 weights,\n",
    "# this will directly load them into your LoRA model without any conversion necessary.\n",
    "lora_model.load_state_dict(base_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  6742609920 total params,\n",
      "  4194304 trainable params,\n",
      "  0.06% of all params are trainable.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from torchtune.modules.peft._utils import get_adapter_params, set_trainable_params\n",
    "\n",
    "# Get all the parameters from the model that are part of the LoRA\n",
    "lora_params = get_adapter_params(lora_model)\n",
    "\n",
    "# Set all the parameters to be trainable\n",
    "set_trainable_params(lora_model, lora_params)\n",
    "\n",
    "# print the trainable parameters\n",
    "total_params = sum([p.numel() for p in lora_model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\n",
    "print(\n",
    "  f\"\"\"\n",
    "  {total_params} total params,\n",
    "  {trainable_params} trainable params,\n",
    "  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.\n",
    "  \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
